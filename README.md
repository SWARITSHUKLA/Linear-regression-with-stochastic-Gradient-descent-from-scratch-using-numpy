# Linear-regression-with-stochastic-Gradient-descent-from-scratch-using-numpy
This is a python code that showcases the use of Stochastic gradient descent in simple Linear regression, Instead of calculating the gradient of the batch at every step we will use one feature & target at a time

# Why do we use stochastic gradient descent?
 Stochastic gradient descent is very fast and yet effective, but the downside is it carries a lot of noise. This issue can be resolved if you just increase the number of iterations,
 To give you an intuition of how efficient  SGD is, I performed 2 million stochastic gradient steps, and it only took 1 minute and 11 seconds.
 PS: my laptop is slow :(
